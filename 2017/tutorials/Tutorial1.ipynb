{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS375 - Tutorial 1\n",
    "### Welcome to tutorial 1! This tutorial will introduce you to basic MongoDB, Tensorflow, and tfutils functions and commands. We will learn how to setup a MongoDB database, and connect, read and write to it. We will then give you a brief refresher on Tensorflow, show you how to read data not only from feed dicts but also TFRecords and build a simple Multi-Layer Perceptron (MLP). We will also show you how to create TFRecord files out of your own data. Finally, we will show you how to use tfutils which puts all pieces together into one coherent framework.\n",
    "\n",
    "## 1.) Using MongoDB with pymongo\n",
    "### First we will learn how to use MongoDB. We will start with setting up a MongoDB database. We assume that you have installed MongoDB following any of the instructions that you can find online. A principled way to setup a MongoDB database is to use a config file. A mongodb.conf file needs to have the following content:\n",
    "\n",
    "\\# Where to store the data.  \n",
    "dbpath=/mongodb/mongodb  \n",
    "\\# Where to log  \n",
    "logpath=/mongodb/mongodb/mongodb.log  \n",
    "\\# IPs and ports your database is broadcasting to (127.0.0.1 is localhost).  \n",
    "bind_ip = 127.0.0.1  \n",
    "port = 24444\n",
    "\n",
    "### That's it! That's all we need to setup and start a MongoDB database. If any of these is not defined MongoDB will fall back to its default settings. We can setup and start a MongoDB database with the following command:\n",
    "`sudo mongod --config=mongodb.conf --quiet&`\n",
    "\n",
    "### Now that we have setup our database, let's try to connect to it using the python interface \"pymongo\". Therefore, we need to specify the port and the host of the database we want to connect to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo as pm\n",
    "import numpy as np\n",
    "\n",
    "port = 24444\n",
    "host = 'localhost'\n",
    "connection = pm.MongoClient(port = port, host = host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test if we have successfully connected to our database let's print out the last 3 database names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(connection.database_names()[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's say we are interested in the contents of \"imagenet\". As all Mongo data is returned in form of a dictionary we can access \"future prediction\" as follows and look up it's collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "database = connection['imagenet']\n",
    "print(database.collection_names())\n",
    "\n",
    "#database2 = connection['task']\n",
    "#print(database2.collection_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's dig deeper and have a look at \"alexnet.files\". As we have learned in the last lecture we have reached the level where we store our experiments uniquely identified by \"exp_id\". So let's find all unique \"exp_id\" in the \"alexnet.files\" collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection = database['alexnet.files']\n",
    "collection.distinct('exp_id')\n",
    "\n",
    "#collection2 = database2['polyfunction']\n",
    "#collection2.distinct('exp_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to find all entries with e. g. \"experiment_1\" we need to query the database for it's \"exp_id\". So we formulate a search query and use it to retrieve the entry as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {'exp_id': 'experiment_1'}\n",
    "entries = collection.find(query)\n",
    "print(entries.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have found 2760 entries with \"exp_id\" \"experiment_1\", some of which belong to training data, and some of which belong to validation data, which I know as I have created those entries with tfutils. Let's have a look at the contents of the first entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that the entry consists of a bunch of keys that store information about the trained model and training results. Now let's say we want to access only those entries that actually contain training results. In order to do this we have to formuate a new database query where we use the \"$exists\" keyword to signal that \"train_results\" has to exist in any entry that gets returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_train = {'exp_id': 'experiment_1', 'train_results': {'$exists' : True}}\n",
    "train_entries = collection.find(query_train)\n",
    "train_entries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, we obtain a list of entries out of which all contain \"train_results\".  We printed the content of the first returned entry and we can observe that although we requested only entries that contain \"train_results\" all of the entries data was returned and not only \"train_results\". If we want Mongo to return only \"train_results\" in each entry we have to use a projection as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_entries = collection.find(query_train, projection = ['train_results'])\n",
    "train_entries[1]\n",
    "\n",
    "#duration_entries = collection.find(query_train,projection = ['duration'])\n",
    "#duration_entries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see we have now successfully retrieved \"train_results\" only, except for the unique identifier \"_id\" which always gets returned. These \"train_results\" entries were saved every couple of iterations while training a model in tfutils and for instance contain the training loss. Using this training data we can now easily construct a vector that contains the training loss over time by using a simple list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_entries = collection.find(query_train, projection = ['train_results'])\n",
    "loss = np.array([results['loss'] for entry in train_entries for results in entry['train_results']])\n",
    "learning_rate = np.array([results['learning_rate'] for entry in train_entries for results in entry['train_results']])\n",
    "print loss.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now go ahead and plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting records into MongoDB as easy as reading data from the database. You simply can simply use the \"insert_one()\" method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# entry has to be a dictionary\n",
    "entry_1 = {'value': 10.1}\n",
    "coll = connection['test_db']['test_coll']\n",
    "coll.insert_one(entry_1)\n",
    "\n",
    "#entry_GR = {'Sarah': 999}\n",
    "#coll.insert_one(entry_GR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And if we now query the \"test_coll\" collection we can see that \"entry_1\" has been stored in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coll = connection['test_db']['test_coll']\n",
    "coll.find({'value': {'$exists' : True}})[0]\n",
    "#coll.find({'Sarah':{'$exists' : True}})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's insert two more entries and sort them by the \"value\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entry_2 = {'value': 4.2}\n",
    "entry_3 = {'value': 8.3}\n",
    "\n",
    "coll = connection['test_db']['test_coll']\n",
    "coll.insert_many([entry_2, entry_3])\n",
    "\n",
    "entries = coll.find({'value': {'$exists' : True}}, projection=['value'])\n",
    "entries = [entry for entry in entries]\n",
    "print('Not sorted:')\n",
    "for entry in entries:\n",
    "    print(entry['value'])\n",
    "\n",
    "entries = coll.find({'value': {'$exists' : True}}, projection=['value']).sort([('value',pm.ASCENDING)])\n",
    "entries = [entry for entry in entries]\n",
    "print('Sorted:')\n",
    "for entry in entries:\n",
    "    print(entry['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's clean up and delete some entries, the collection and database to conclude the tutorial on pymongo and MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove entry with field 'value' = 10.1\n",
    "#coll.remove({'value' : 10.1}, {'justOne': True})\n",
    "coll.delete_one({'value' : 10.1})\n",
    "coll.delete_one({'Sarah' : 999})\n",
    "\n",
    "# remove collection\n",
    "connection['test_db'].drop_collection('test_coll')\n",
    "\n",
    "# remove database\n",
    "connection.drop_database('test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Creating a simple model in Tensorflow\n",
    "### We will now move on to a brief exercise on Tensorflow. We will keep this section short since most of you should be familiar with Tensorflow by now. In the following, we will talk about how to create a Tensorflow graph, execute the graph in a session, variables and variable scopes, placeholders and feed dicts.\n",
    "\n",
    "### First we will define a 3 layer MLP with 3 input neurons, 20 hidden neurons, and 3 output neurons in Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# input placeholder\n",
    "input_placeholder = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "# trainable variables randomly initialized\n",
    "W1 = tf.Variable(tf.random_normal(shape=[3,20]), tf.float32)\n",
    "b1 = tf.Variable(tf.random_normal(shape=[20,]), tf.float32)\n",
    "W2 = tf.Variable(tf.random_normal(shape=[20,3]), tf.float32)\n",
    "b2 = tf.Variable(tf.random_normal(shape=[3,]), tf.float32)\n",
    "# hidden layer\n",
    "h = tf.nn.sigmoid(tf.matmul(input_placeholder, W1) + b1)\n",
    "# output\n",
    "out = tf.matmul(h, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run one forward pass of this randomly initialized MLP with some random data we first have to create a session and initialize all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "# creates a session and allocates memory on GPU 1\n",
    "sess = tf.Session()\n",
    "# initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we can execute the forward pass with a call to the session's \"run()\" method, while specifying the output targets and feeding the input data through the feed_dict parameter to the placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some random input and run forward pass on it\n",
    "inp = np.random.rand(1, 3).astype(np.float32)\n",
    "result = sess.run(out, feed_dict={input_placeholder: inp})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.size(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's train this MLP to regress some arbitrary function \"y\". In order to do that we need to define a loss and an optimizer that minimizes that loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the input x and output y\n",
    "x = np.random.rand(1000,3).astype(np.float32)\n",
    "y = x ** 2 + 3\n",
    "# labels placeholder\n",
    "labels_placeholder = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "# simple l2-loss\n",
    "loss = tf.nn.l2_loss(out - labels_placeholder)\n",
    "# Gradient descent optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train for 1,000 steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "for i in trange(1000):\n",
    "    idx = np.random.RandomState(0).permutation(1000)[:32]\n",
    "    inputs = x[idx]\n",
    "    labels = y[idx]\n",
    "    sess.run(train_step, feed_dict={input_placeholder: inputs, labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and evaluate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = np.random.rand(1, 3).astype(np.float32)\n",
    "ground_truth = inp ** 2 + 3\n",
    "prediction = sess.run(out, feed_dict={input_placeholder: inp})\n",
    "print('Ground truth', ground_truth) \n",
    "print('Prediction', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking good! Our model approximates the function \"y\" pretty well! However there's a better way to feed data to your tensorflow model which comes in particularly handy when working with large-scale datasets: TFRecords!\n",
    "\n",
    "## 3.) Using TFRecords\n",
    "\n",
    "### TFRecord is a file format for Tensorflow used to store data in its flattened, serialized byte format. Any type of data can be stored in TFRecords as long as its serializable. Tensorflow implements a TFRecordReader that is able to read this file format into a Tensorflow graph. In Tensorflow queues are used to allow for an asynchronous data reading and model execution. Queues also enables the use of multiple threads for data reading. TFRecords store data sequentially, and only allow for it to be read in sequential order. Random access is not possible. This makes the data loading from disk super fast but makes data randomization impossible. Thus, in order to randomize the input data as it is usually necessary to train machine learning models, there needs to be a separate postprocessing stage that takes care of the randomization. The nice thing about using queues is that one can simply shuffle the data after loading and before handing it over to the model for the feedforward pass. \n",
    "\n",
    "### Thus in summary, TFRecords together with file and data queues provide an elegant framework to load large amounts of data in a short period of time which is crucial at a time where GPUs are no longer the bottleneck for training large-scale models and the training speed of a network is dependent on the data loading speed from hard drives. \n",
    "\n",
    "### In the following, we will first show you how to write TFRecords. So let's construct some arbitrary data and save it on disk in the tfrecords format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arbitrary data with some arbitrary attributes\n",
    "data = {\n",
    "    'idx': np.array(range(100)).astype(np.int32),\n",
    "    'values': np.array(range(100)).astype(np.int32)[::-1].astype(np.float32),\n",
    "    'labels': np.array(['large', 'scale', 'neural', 'network', 'modeling', 'for', 'neuroscience', 'is', 'the', 'best!'] * 10)\n",
    "    }\n",
    "\n",
    "# Make sure all attributes are of the same length\n",
    "for k in data:\n",
    "    assert len(data[k]) == len(data[data.keys()[0]])\n",
    "\n",
    "# We are storing our data as byte strings\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "# Create tfrecord file\n",
    "file_path = '/mnt/fs0/mrowca/notebooks/mrowca/myfile.tfrecords'\n",
    "writer = tf.python_io.TFRecordWriter(file_path)\n",
    "for i in range(len(data[data.keys()[0]])):\n",
    "    # Construct the datum to be written\n",
    "    feature = {}\n",
    "    for k in data:\n",
    "        # serialize the data\n",
    "        feature[k] = _bytes_feature(data[k][i].tostring())\n",
    "    datum = tf.train.Example(features = tf.train.Features(feature = feature))\n",
    "    # Write the data\n",
    "    writer.write(datum.SerializeToString())\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's read the data that we have written. There are two ways to do that. One that requires you to use a session as the \"tf.TFRecordReader()\" is part of the Tensorflow graph, and one that you can use to view the data stored inside of TFRecords without constructing a graph: \"tf.python_io.tf_record_iterator()\". While the first method is actually used when running large-scale experiments as it can be easily parallelized using multiple queue runners which enables faster loading, the second method is useful for debugging purposes. So let's use \"tf.python_io.tf_record_iterator()\" to have a brief look at our newly written TFRecords file to see if it has been written correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the tfrecord file\n",
    "file_path = '/mnt/fs0/mrowca/notebooks/mrowca/myfile.tfrecords'\n",
    "tfrec = tf.python_io.tf_record_iterator(path=file_path)\n",
    "datum = tf.train.Example()\n",
    "# Go to the first entry and decode it\n",
    "datum.ParseFromString(tfrec.next())\n",
    "# Print the attribute names in this record\n",
    "print('Attributes:')\n",
    "print(datum.features.feature.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access the attributes and decode them to the correct data type\n",
    "idx = np.fromstring(datum.features.feature['idx'].bytes_list.value[0], dtype=np.int32)\n",
    "value = np.fromstring(datum.features.feature['values'].bytes_list.value[0], dtype=np.float32)\n",
    "label = datum.features.feature['labels'].bytes_list.value[0]\n",
    "print('idx:', idx)\n",
    "print('value:', value)\n",
    "print('label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything looks correct for the first entry. If we would want to examine the second entry we would simply call \"tfrec.next()\" again and we would get the second entry and so on. We won't do that here though and instead assume the tfrecord has been correctly written. \n",
    "\n",
    "### Now let's use the first method \"tf.TFRecordReader()\" to read data into a Tensorflow graph that simply outputs the data. We will need \"tf.TFRecordReader()\" as well as a file queue for that which we can use as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 1\n",
    "# construct the filename queue that holds all file names in our case only \"myfile.tfrecords\"\n",
    "file_path = '/mnt/fs0/mrowca/notebooks/mrowca/myfile.tfrecords'\n",
    "filename_queue = tf.train.string_input_producer([file_path])\n",
    "\n",
    "# setup the reader and read the first batch_size examples\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read_up_to(filename_queue, batch_size)\n",
    "\n",
    "# decode the example\n",
    "features = tf.parse_example(\n",
    "        serialized_example, features={\n",
    "        'idx': tf.FixedLenFeature([], tf.string),\n",
    "        'values': tf.FixedLenFeature([], tf.string),\n",
    "        'labels': tf.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "idx_record = tf.decode_raw(features['idx'], tf.int32)\n",
    "values_record = tf.decode_raw(features['values'], tf.float32)\n",
    "labels_record = features['labels']\n",
    "\n",
    "# enqueue the example onto a random shuffle queue for shuffling and add a queue runner that takes care of this\n",
    "queue = tf.RandomShuffleQueue(dtypes=[tf.int32, tf.float32, tf.string],\n",
    "                              capacity=10, \n",
    "                              min_after_dequeue=5, \n",
    "                              seed = 5)\n",
    "enqueue_op = queue.enqueue([idx_record, values_record, labels_record])\n",
    "tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(queue, [enqueue_op]))\n",
    "\n",
    "# define the output as dequeuing from the random shuffle queue, so we can use it as a target in sess.run\n",
    "output = queue.dequeue()\n",
    "\n",
    "# initialize variables and start queue runners\n",
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)                                             \n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "# run forward pass \n",
    "data = sess.run(output)\n",
    "\n",
    "# print results\n",
    "print('idx:', data[0][0])\n",
    "print('value:', data[1][0])\n",
    "print('label:', data[2][0])\n",
    "\n",
    "# stop the queue runners\n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To summarize, we first created a filename_queue that holds the filenames of all TFRecords we want to load data from. We then used a TFRecordReader to read one example off a TFRecords file on the filename_queue. We decoded this example into its original attributes. We enqueued those attributes onto another queue that we used to shuffle our data using another queue runner. We then dequeued an example from the queue as output, which concluded the construction of our graph. To run the graph, we initialized all variables and queue runners and called the \"run()\" method, which allowed us to retrieve the outputs and print them out. Finally, after we were done, we had to stop the queue runner threads.  TFUtils implements exactly this procedure and we will have a closer look how to use it in the next section.\n",
    "\n",
    "## 4.) Using TFUtils\n",
    "\n",
    "### TFUtils, in one way or another, uses everything we have discussed so far. It is a great framework to keep track of your experiments in a database. You can train a model, restore its weights, test it, modify it, extract intermediate features, or do all at once while TFUtils is keeping track of the developmental trajectory of your network. \n",
    "\n",
    "### In the following, we will first show you how to train a model in TFUtils. Therefore, let's use the standard MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from tfutils import base, data, optimizer, utils\n",
    "\n",
    "# delete exp1\n",
    "connection['mnist']['simple.files'].delete_many({'exp_id' : 'exp1'})\n",
    "\n",
    "def mnist_model(inputs, train=True, **kwargs):\n",
    "    # trainable variables randomly initialized\n",
    "    with tf.variable_scope(\"mnist\"):\n",
    "        W1 = tf.get_variable('W1', [784,128], tf.float32, tf.random_normal_initializer(stddev=0.1))\n",
    "        b1 = tf.get_variable('b1', [128], tf.float32, tf.random_normal_initializer(stddev=0.1))\n",
    "        W2 = tf.get_variable('W2', [128,32], tf.float32, tf.random_normal_initializer(stddev=0.1))\n",
    "        b2 = tf.get_variable('b2', [32], tf.float32, tf.random_normal_initializer(stddev=0.1))\n",
    "        W3 = tf.get_variable('W3', [32,10], tf.float32, tf.random_normal_initializer(stddev=0.1))\n",
    "        b3 = tf.get_variable('b3', [10], tf.float32, tf.random_normal_initializer(stddev=0.1))\n",
    "        \n",
    "        # hidden layers\n",
    "        h1 = tf.nn.sigmoid(tf.matmul(inputs['images'], W1) + b1, name='hidden1')\n",
    "        h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2, name='hidden2')\n",
    "        # output\n",
    "        output = tf.matmul(h2, W3) + b3\n",
    "\n",
    "    return output, {}\n",
    "\n",
    "params = {}\n",
    "\n",
    "params['load_params'] = {\n",
    "    'do_restore': False}\n",
    "\n",
    "params['save_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'simple',\n",
    "    'exp_id': 'exp1',\n",
    "    'save_valid_freq': 200,\n",
    "    'save_filters_freq': 100,\n",
    "    'cache_filters_freq': 100}\n",
    "\n",
    "params['train_params'] = {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 256,\n",
    "                    'group': 'train',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'random',\n",
    "                     'batch_size': 256},\n",
    "    'num_steps': 100}\n",
    "\n",
    "params['model_params'] = {\n",
    "    'func': mnist_model} \n",
    "\n",
    "params['learning_rate_params'] = {\n",
    "    'learning_rate': 0.5,\n",
    "    'decay_steps': 500,\n",
    "    'decay_rate': 0.95,\n",
    "    'staircase': True}\n",
    "\n",
    "params['optimizer_params'] = {\n",
    "    'func': optimizer.ClipOptimizer,\n",
    "    'optimizer_class': tf.train.MomentumOptimizer,\n",
    "    'momentum': 0.9,\n",
    "    'clip': True,\n",
    "}\n",
    "\n",
    "params['loss_params'] = {\n",
    "    'targets': ['labels'],\n",
    "    'loss_per_case_func': tf.nn.sparse_softmax_cross_entropy_with_logits,\n",
    "    'agg_func': tf.reduce_mean\n",
    "}\n",
    "\n",
    "params['skip_check'] = True\n",
    "\n",
    "base.train_from_params(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, all you need to train a model in TFUtils is to define \"params\" and call \"base.train_from_params(**params\"). TFUtils then executes the specified experiment, in this case training MNIST on a 10-way digit recognition task. As you can see the loss decreases, and at the end of the experiment the trained model is saved in the database as specified by \"save_params\". Now let's load the trained model from the database and test it on the validation set using TFUtils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top1_func(inputs, outputs): \n",
    "    return {'top1': tf.nn.in_top_k(outputs, inputs['labels'], 1)}\n",
    "\n",
    "def online_agg(agg_res, res, step):\n",
    "    \"\"\"\n",
    "    Appends the value for each key\n",
    "    \"\"\"\n",
    "    if agg_res is None:\n",
    "        agg_res = {k: [] for k in res}\n",
    "    for k, v in res.items():\n",
    "        agg_res[k].append(v)\n",
    "    return agg_res\n",
    "\n",
    "def agg_mean(x):\n",
    "    return {k: np.mean(v) for k, v in x.items()}\n",
    "\n",
    "params = {}\n",
    "\n",
    "params['load_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'simple',\n",
    "    'exp_id': 'exp1',\n",
    "    'do_restore': True}\n",
    "\n",
    "params['validation_params'] = {'valid0': {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 100,\n",
    "                    'group': 'test',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'fifo',\n",
    "                     'batch_size': 100},\n",
    "    'targets': {'func': top1_func},\n",
    "    'num_steps': 100,\n",
    "    'agg_func': agg_mean,\n",
    "    'online_agg_func': online_agg,}}\n",
    "\n",
    "params['model_params'] = {\n",
    "    'func': mnist_model}\n",
    "\n",
    "params['skip_check'] = True\n",
    "\n",
    "base.test_from_params(**params)\n",
    "\n",
    "# Extract record from database\n",
    "q_val = {'exp_id' : 'exp1', 'validation_results' : {'$exists' : True}, 'validates': {'$exists': True}}\n",
    "val_steps = connection['mnist']['simple.files'].find(q_val, projection = ['validation_results'])\n",
    "top1 = [val_steps[i]['validation_results']['valid0']['top1'] \n",
    "        for i in range(val_steps.count())]\n",
    "print(top1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of course you can also put train and validation together. So let's restore the training, train some more and validate every 100 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['load_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'simple',\n",
    "    'exp_id': 'exp1',\n",
    "    'do_restore': True}\n",
    "\n",
    "params['save_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'simple',\n",
    "    'exp_id': 'exp1',\n",
    "    'save_valid_freq': 100,\n",
    "    'save_filters_freq': 100,\n",
    "    'cache_filters_freq': 100}\n",
    "\n",
    "params['train_params'] = {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 256,\n",
    "                    'group': 'train',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'random',\n",
    "                     'batch_size': 256},\n",
    "    'num_steps': 3000}\n",
    "\n",
    "params['learning_rate_params'] = {\n",
    "    'learning_rate': 0.5,\n",
    "    'decay_steps': 500,\n",
    "    'decay_rate': 0.95,\n",
    "    'staircase': True}\n",
    "\n",
    "params['optimizer_params'] = {\n",
    "    'func': optimizer.ClipOptimizer,\n",
    "    'optimizer_class': tf.train.MomentumOptimizer,\n",
    "    'momentum': 0.9,\n",
    "    'clip': True,\n",
    "}\n",
    "\n",
    "params['loss_params'] = {\n",
    "    'targets': ['labels'],\n",
    "    'loss_per_case_func': tf.nn.sparse_softmax_cross_entropy_with_logits,\n",
    "    'agg_func': tf.reduce_mean\n",
    "}\n",
    "\n",
    "params['validation_params'] = {'valid0': {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 100,\n",
    "                    'group': 'test',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'fifo',\n",
    "                     'batch_size': 100},\n",
    "    'targets': {'func': top1_func},\n",
    "    'num_steps': 100,\n",
    "    'agg_func': agg_mean,\n",
    "    'online_agg_func': online_agg,}}\n",
    "\n",
    "params['model_params'] = {\n",
    "    'func': mnist_model}\n",
    "\n",
    "params['skip_check'] = True\n",
    "\n",
    "base.train_from_params(**params)\n",
    "\n",
    "# Extract record from database\n",
    "q_val = {'exp_id' : 'exp1', 'validation_results' : {'$exists' : True}}\n",
    "val_steps = connection['mnist']['simple.files'].find(q_val, projection = ['validation_results'])\n",
    "top1 = [val_steps[i]['validation_results']['valid0']['top1'] \n",
    "        for i in range(val_steps.count())]\n",
    "print(top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(top1)\n",
    "plt.grid()\n",
    "plt.title('MNIST Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! We have reached 98 % accuracy. Let's add another hidden layer and see if we can improve the result by keeping our already trained layers fixed and only training the new layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mnist_model_with_additional_layer(inputs, train=True, **kwargs):\n",
    "    # not trainable variables; Note how we set trainable=False\n",
    "    with tf.variable_scope(\"mnist\"):\n",
    "        W1 = tf.get_variable('W1', [784,128], tf.float32, tf.random_normal_initializer(stddev=0.1), trainable=False)\n",
    "        b1 = tf.get_variable('b1', [128], tf.float32, tf.random_normal_initializer(stddev=0.1), trainable=False)\n",
    "        W2 = tf.get_variable('W2', [128,32], tf.float32, tf.random_normal_initializer(stddev=0.1), trainable=False)\n",
    "        b2 = tf.get_variable('b2', [32], tf.float32, tf.random_normal_initializer(stddev=0.1), trainable=False)\n",
    "        W3 = tf.get_variable('W3', [32,10], tf.float32, tf.random_normal_initializer(stddev=0.1), trainable=False)\n",
    "        b3 = tf.get_variable('b3', [10], tf.float32, tf.random_normal_initializer(stddev=0.1), trainable=False) \n",
    "        \n",
    "        # hidden layers\n",
    "        h1 = tf.nn.sigmoid(tf.matmul(inputs['images'], W1) + b1, name='hidden1')\n",
    "        h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2, name='hidden2')\n",
    "    \n",
    "        output = tf.nn.sigmoid(tf.matmul(h2, W3) + b3, name='hidden3')\n",
    "    \n",
    "        # new layer, trainable, randomly initialized\n",
    "        W4 = tf.get_variable('W4', [10,10], tf.float32, tf.random_normal_initializer(stddev=0.1))\n",
    "        b4 = tf.get_variable('b4', [10], tf.float32, tf.random_normal_initializer(stddev=0.1))\n",
    "        output = tf.matmul(output, W4) + b4\n",
    "\n",
    "    return output, {}\n",
    "\n",
    "params = {}\n",
    "\n",
    "params['load_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'simple',\n",
    "    'exp_id': 'exp1',\n",
    "    'do_restore': True}\n",
    "\n",
    "params['save_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'simple',\n",
    "    'exp_id': 'exp1',\n",
    "    'save_valid_freq': 100,\n",
    "    'save_filters_freq': 100,\n",
    "    'cache_filters_freq': 100}\n",
    "\n",
    "params['train_params'] = {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 256,\n",
    "                    'group': 'train',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'random',\n",
    "                     'batch_size': 256},\n",
    "    'num_steps': 5000}\n",
    "\n",
    "params['learning_rate_params'] = {\n",
    "    'learning_rate': 0.5,\n",
    "    'decay_steps': 500,\n",
    "    'decay_rate': 0.95,\n",
    "    'staircase': True}\n",
    "\n",
    "params['optimizer_params'] = {\n",
    "    'func': optimizer.ClipOptimizer,\n",
    "    'optimizer_class': tf.train.MomentumOptimizer,\n",
    "    'momentum': 0.9,\n",
    "    'clip': True,\n",
    "}\n",
    "\n",
    "params['loss_params'] = {\n",
    "    'targets': ['labels'],\n",
    "    'loss_per_case_func': tf.nn.sparse_softmax_cross_entropy_with_logits,\n",
    "    'agg_func': tf.reduce_mean\n",
    "}\n",
    "\n",
    "params['validation_params'] = {'valid0': {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 100,\n",
    "                    'group': 'test',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'fifo',\n",
    "                     'batch_size': 100},\n",
    "    'targets': {'func': top1_func},\n",
    "    'num_steps': 100,\n",
    "    'agg_func': agg_mean,\n",
    "    'online_agg_func': online_agg,}}\n",
    "\n",
    "params['model_params'] = {\n",
    "    'func': mnist_model_with_additional_layer}\n",
    "\n",
    "params['skip_check'] = True\n",
    "\n",
    "base.train_from_params(**params)\n",
    "\n",
    "# Extract record from database\n",
    "q_val = {'exp_id' : 'exp1', 'validation_results' : {'$exists' : True}}\n",
    "val_steps = connection['mnist']['simple.files'].find(q_val, projection = ['validation_results'])\n",
    "top1 = [val_steps[i]['validation_results']['valid0']['top1'] \n",
    "        for i in range(val_steps.count())]\n",
    "print(top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(top1)\n",
    "plt.grid()\n",
    "plt.title('MNIST Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This clearly did not work. Although the performance keeps increasing after evaluation step 30 when the new layer was added, it seems to level off at 94 %. This was example only to illustrate how you can reload weights from your trained model, keep the trained layers fixed, add new layers and only train the newly added layers. So let's not do it again. \n",
    "\n",
    "### The last thing that we will show you in TFUtils is how to extract features from the model graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_extraction_target(inputs, outputs, to_extract, **loss_params):\n",
    "    names = [[x.name for x in op.values()] for op in tf.get_default_graph().get_operations()]\n",
    "    names = [y for x in names for y in x]\n",
    "\n",
    "    #print('nodes in graph: ', names)\n",
    "    r = re.compile(r'__GPU__\\d/')\n",
    "    _targets = defaultdict(list)\n",
    "\n",
    "    for name in names:\n",
    "        name_without_gpu_prefix = r.sub('', name)\n",
    "        for save_name, actual_name in to_extract.items():\n",
    "            if actual_name in name_without_gpu_prefix:\n",
    "                tensor = tf.get_default_graph().get_tensor_by_name(name)\n",
    "                _targets[save_name].append(tensor)\n",
    "\n",
    "    targets = {k: tf.concat(v, axis=0) for k, v in _targets.items()}\n",
    "    return targets\n",
    "\n",
    "def online_agg_no_mean(agg_res, res, step):\n",
    "    \"\"\"\n",
    "    Appends the value for each key\n",
    "    \"\"\"\n",
    "    if agg_res is None:\n",
    "        agg_res = {k: [] for k in res}\n",
    "    for k, v in res.items():\n",
    "        agg_res[k].append(v)\n",
    "    return agg_res\n",
    "\n",
    "def agg_no_mean(x):\n",
    "    return {k: v for k, v in x.items()}\n",
    "\n",
    "params = {}\n",
    "\n",
    "params['load_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'simple',\n",
    "    'exp_id': 'exp1',\n",
    "    'do_restore': True}\n",
    "\n",
    "params['validation_params'] = {'valid0': {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 1,\n",
    "                    'group': 'test',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'fifo',\n",
    "                     'batch_size': 1},\n",
    "    'targets': {'func': get_extraction_target,\n",
    "               'to_extract': {\n",
    "                   'W2': 'mnist/W2:0',\n",
    "                   'hidden2': 'model_0/validation/valid0/mnist/hidden2:0'}},\n",
    "    'num_steps': 1,\n",
    "    'agg_func': agg_no_mean, # we do not want to mean the features that we extract\n",
    "    'online_agg_func': online_agg_no_mean,}}\n",
    "\n",
    "params['model_params'] = {\n",
    "    'func': mnist_model}\n",
    "\n",
    "params['skip_check'] = True\n",
    "\n",
    "base.test_from_params(**params)\n",
    "\n",
    "# print shape of retrieved weights and activations\n",
    "q_val = {'exp_id' : 'exp1', 'validation_results' : {'$exists' : True}, 'validates': {'$exists' : True}}\n",
    "val_steps = connection['mnist']['simple.files'].find(q_val, projection = ['validation_results'])\n",
    "idx = val_steps.count() - 1\n",
    "print('W2', np.array(val_steps[idx]['validation_results']['valid0']['W2'])[0].shape)\n",
    "print('hidden2', np.array(val_steps[idx]['validation_results']['valid0']['hidden2'])[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This concludes the tutorial on TFUtils. We will now diverge a little bit from deep learning and introduce some useful tools to analyze neural data, such as tabular, scikit-learn and dldata.\n",
    "\n",
    "## 5.) Working with the tabular meta data from dldata\n",
    "### During your analysis of the neural data, you will be working with our lab-internal python package \"dldata\". \"dldata\" contains stimuli in the form of images and the neural responses of a macaque to those images accompanied by some meta data. The meta data contains information about the composition of the stimuli such as which category is depicted or where the object is located. Meta data itself is a tabarray which allows for an easy way of subsetting the data. So let's first load the meta data and the IT features from a pickle file that we have created and have put on your instances and then learn how to subsample the data by using the meta data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load from tfrecords\n",
    "import cPickle\n",
    "data_path = '/home/mrowca/neural_data.pkl' # CHANGE THIS TO '/datasets/neural_data/neural_data.pkl'\n",
    "with open(data_path) as f:\n",
    "    data = cPickle.load(f)\n",
    "meta = data['meta']\n",
    "IT_features = data['IT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's list all the fields within meta..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta.dtype.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...and have a look at the contents of 'obj':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta['obj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get all unique entries we can use np.unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(meta['obj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how many unique 'obj' there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.unique(meta['obj']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at the unique 'categories':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(meta['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's say we want to find all 'Cars' in 'obj' now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(meta[meta['category'] == 'Cars']['obj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also use intersections and unions to subselect our data. Let's say we want to find all 'Cars' and all 'Faces' in 'obj':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(meta[(meta['category'] == 'Cars') | (meta['category'] == 'Faces')]['obj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's find out how many images have an object on the left side of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len((meta['ty'] < 0).nonzero()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also combine 'category' with 'ty' and look for e. g. all 'Cars' with at least one instance on the left side of the screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(meta[(meta['category'] == 'Cars') & (meta['ty'] < 0)]['obj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To conclude you can combine the fields in a tabarray in any possible imaginable way you want using logical operators to subselect the data as you need it. This enables us to save and query data in a structured way.\n",
    "\n",
    "## 6.) Training and testing a classifier with scikit-learn and dldata\n",
    "### During the analysis of neural responses it useful to see how well the recorded features can be used to disentangle object categories using a simple linear classifier. In the following we will thus show you how to train a linear support vector classifier on top of IT features to perform a basic 8-way classification task using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Create a random train / test split and \n",
    "# use 75 % of the V0 data for training and the rest for testing\n",
    "v0_inds = (meta['var'] == 'V0').nonzero()[0]\n",
    "n = len(v0_inds)\n",
    "train_inds = np.random.RandomState(0).permutation(n)[: int(3 * n / 4)]\n",
    "test_inds = np.random.RandomState(0).permutation(n)[int(3 * n / 4): ]\n",
    "train_inds = v0_inds[train_inds]\n",
    "test_inds = v0_inds[test_inds]\n",
    "\n",
    "# Subselect the train and test IT features\n",
    "train_features = IT_features[train_inds]\n",
    "test_features = IT_features[test_inds]\n",
    "\n",
    "# Subselect the train and test category labels\n",
    "train_labels = meta['category'][train_inds]\n",
    "test_labels = meta['category'][test_inds]\n",
    "\n",
    "# Convert labels to integers\n",
    "cat_id = dict([(category, i) for i, category in enumerate(np.unique(meta['category']))])\n",
    "train_labels = np.array([cat_id[label] for label in train_labels])\n",
    "test_labels = np.array([cat_id[label] for label in test_labels])\n",
    "\n",
    "# Train LinearSVC (Linear Support Vector Classifier)\n",
    "cls = LinearSVC(C = 5e-3)\n",
    "cls.fit(train_features, train_labels)\n",
    "\n",
    "# Test LinearSVC\n",
    "prediction = cls.predict(test_features)\n",
    "accuracy = np.sum(prediction == test_labels) / len(test_labels) * 100\n",
    "\n",
    "# Print results\n",
    "print('Overall accuracy of IT features on 8-way classification task: %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see a significant amount of code is needed to perform a classification experiment on only one split! All of this code is abstracted away in dldata to a simple function called \"compute_metric_base\" which takes care of everything we need including the creation of multiple train / test splits. This means that we can setup the same experiment with a simple spec that we then pass on to \"compute_metric_base\" to perform our classification experiment with multiple splits as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dldata.metrics.utils import compute_metric_base\n",
    "# Definition of classification experiment\n",
    "category_eval_spec = {\n",
    "    'npc_train': None,\n",
    "    'npc_test': 2,\n",
    "    'num_splits': 20,\n",
    "    'npc_validate': 0,\n",
    "    'metric_screen': 'classifier',\n",
    "    'metric_labels': None,\n",
    "    'metric_kwargs': {'model_type': 'svm.LinearSVC',\n",
    "                      'model_kwargs': {'C':5e-3}\n",
    "                     },\n",
    "    'labelfunc': 'category',\n",
    "    'train_q': {'var': ['V0']},\n",
    "    'test_q': {'var': ['V0']},\n",
    "    'split_by': 'obj'\n",
    "}\n",
    "# Execute classification experiment\n",
    "res = compute_metric_base(IT_features, meta, category_eval_spec)\n",
    "\n",
    "# Print results\n",
    "print('Overall accuracy of IT features on 8-way classification task: %.2f%%' % \\\n",
    "      ((np.array(res['result_summary']['accbal']).mean(0) - 0.5) * 2.0 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'compute_metric_base' also returns a bunch of other evaluation metrics such as dprime or the confusion matrix. Feel free to have a look at res['results_summary'] to see which metrics are returned. To plot the confusion matrix for instance, we can execute the following straight forward commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "m = fig.gca().matshow(np.array(res['result_summary']['cms']).mean(2))\n",
    "plt.xticks(range(8), res['result_summary']['labelset'])\n",
    "plt.yticks(range(8), res['result_summary']['labelset'])\n",
    "plt.colorbar(m)\n",
    "plt.title('8-way categorization task - across category confusion matrix')\n",
    "ax = plt.gca()\n",
    "ax.xaxis.tick_bottom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example hopefully made it clear why it is simpler to use \"compute_metric_base\" for simple classification or regression experiments. Nonetheless, we wanted to show you the inner workings of \"compute_metric_base\", so you can implement your own machine learning tools with scikit-learn in case that the particular functionality you are looking for is not implemented in \"compute_metric_base\".\n",
    "\n",
    "## 7.) Putting everything together\n",
    "### At this point, you should have learned everything needed to setup a large-scale deep learning experiment and be able to understand most of the code in assignment 1 which is a great example of how everything that we have discussed fits together. Please have a look at the code of assignment 1. Conceptually, we have setup assignment 1 in the following way:  \n",
    "### 1.) We generated TFRecord files for the ImageNet dataset and the neural data including all of the meta data needed and wrote dataproviders that read those TFRecords into TFUtils in \"dataprovider.py\".  \n",
    "### 2.) We then wrote the \"train_imagenet.py\" script which specifies a TFUtils training experiment to train AlexNet on ImageNet saving the training results into our database. While the experiment is running the accuracy on the ImageNet classification task is evaluated and stored into the database to see if the model is training correctly.\n",
    "### 3.) We then created \"test_imagenet.py\" which pulls the pretrained AlexNet model from the database and evaluates it using among others \"compute_metric_base\" from dldata with the help of the meta data that is read from the TFRecords and piped through the network. Essentially, we aggregate all of our network activations for all neural stimuli in our \"agg_func\" \"neural_analysis\" and perform our classification and regression experiments on the aggregated data. We store the evaluation results in the database.\n",
    "### 4.) Finally, we pull the evaluation results from the database in our Jupyter notebook, and evaluate and visualize them. \n",
    "### In general, this is how most of the experiments are done in our lab. TFRecords allow us to read data fast. TFUtils helps us to keep organized. And Jupyter notebooks are a great and interactive way to visualize results.\n",
    "### This concludes this tutorial on CNNs and Neural data. Good luck on assignment 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
